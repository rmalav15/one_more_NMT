{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow NMT implementation: Will Cover \n",
    "- bi-direction GRU\n",
    "- Attention Model\n",
    "- Language Model\n",
    "- Highway Networks (Probably)\n",
    "- Tensorboard Functionalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Change this ipynb into a project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nmt_utils.helper import data_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() #Clears the default graph stack and resets the global default graph.\n",
    "sess = tf.InteractiveSession() #initializes a tensorflow session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class opt:\n",
    "    #source_vocab_size = None,\n",
    "    #dest_vocab_size  =None,\n",
    "    source_embedding_size = 1000\n",
    "    dest_embedding_size = 1000\n",
    "    source_training_file = \"/media/user/069A453E9A452B8D/Ram/projectsData/nmt/train.en\"\n",
    "    dest_training_file = \"/media/user/069A453E9A452B8D/Ram/projectsData/nmt/train.vi\"\n",
    "    source_val_file = \"/media/user/069A453E9A452B8D/Ram/projectsData/nmt/tst2012.en\"\n",
    "    dest_val_file = \"/media/user/069A453E9A452B8D/Ram/projectsData/nmt/tst2012.vi\"\n",
    "    source_vocab_file = \"/media/user/069A453E9A452B8D/Ram/projectsData/nmt/vocab.en\"\n",
    "    dest_vocab_file = \"/media/user/069A453E9A452B8D/Ram/projectsData/nmt/vocab.vi\"\n",
    "    \n",
    "#     source_max_length = None\n",
    "#     dest_max_length = None    \n",
    "#     source_vocab_size= None\n",
    "#     dest_vocab_size=None\n",
    "#     batch_size = None\n",
    "#     source_vocab= None\n",
    "#     dest_vocab = None\n",
    "    \n",
    "    #encoder_hidden_units = 2\n",
    "    encoder_hiddent_units_size = [256,256]\n",
    "    \n",
    "    #decoder_hidden_units = 2\n",
    "    decoder_hidden_units_size = [512, 256]\n",
    "    \n",
    "    share_vocab = False\n",
    "    time_major = False\n",
    "    \n",
    "    encoder_state_to_decoder = False\n",
    "    \n",
    "    attention_depth = 256 ## attention Ai\n",
    "    attention_layer_size = 128 ###\n",
    "    \n",
    "    mode = \"train\" ## or \"infer\"\n",
    "    \n",
    "data = data_helper(opt)   \n",
    "\n",
    "UNK = \"<unk>\"\n",
    "SOS = \"<s>\"\n",
    "EOS = \"</s>\"\n",
    "UNK_ID = 0\n",
    "\n",
    "#opt.source_vocab_file, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('placeholders') as scope:\n",
    "    encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "    encoder_inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_inputs_length')\n",
    "\n",
    "    decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')\n",
    "    decoder_targets_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='decoder_targets_length')\n",
    "    \n",
    "    #source_start_token = tf.placeholder(shape=(1,), dtype=tf.int32, name='start_token')\n",
    "    #source_end_token = tf.palceholder(shape=(1,), dtype=tf.int32, name='end_token')\n",
    "    \n",
    "    dest_start_token = tf.placeholder(shape=(1,), dtype=tf.int32, name='start_token')\n",
    "    dest_end_token = tf.palceholder(shape=(1,), dtype=tf.int32, name='end_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_max_length = None\n",
    "dest_max_length = None    \n",
    "\n",
    "source_vocab_size= None\n",
    "dest_vocab_size=None\n",
    "\n",
    "batch_size = None\n",
    "\n",
    "source_vocab= None\n",
    "dest_vocab = None\n",
    "\n",
    "source_max_length, batch_size = tf.unstack(tf.shape(encoder_inputs))\n",
    "dest_max_length, _ = tf.unstack(tf.shape(decoder_targets))\n",
    "\n",
    "source_vocab, source_vocab_size = data.load_vocab(opt.source_vocab_file)\n",
    "dest_vocab, dest_vocab_size = data.load_vocab(opt.dest_vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Must specify vocab size and embedding dimension when notreusing. Got vocab_size=None and embed_dim=1000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f1d34b576342>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"embed\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     input_embeddings = tf.contrib.layers.embed_sequence(encoder_inputs, vocab_size = data.source_vocab_size,\n\u001b[0;32m----> 3\u001b[0;31m                                 embed_dim=opt.source_embedding_size)\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     decoder_targets_embeddings = tf.contrib.layers.embed_sequence(decoder_targets, vocab_size = data.source_vocab_size,\n",
      "\u001b[0;32m/home/user/genv/local/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/encoders.pyc\u001b[0m in \u001b[0;36membed_sequence\u001b[0;34m(ids, vocab_size, embed_dim, unique, initializer, regularizer, trainable, scope, reuse)\u001b[0m\n\u001b[1;32m    128\u001b[0m     raise ValueError('Must specify vocab size and embedding dimension when not'\n\u001b[1;32m    129\u001b[0m                      'reusing. Got vocab_size=%s and embed_dim=%s' % (\n\u001b[0;32m--> 130\u001b[0;31m                          vocab_size, embed_dim))\n\u001b[0m\u001b[1;32m    131\u001b[0m   with variable_scope.variable_scope(\n\u001b[1;32m    132\u001b[0m       scope, 'EmbedSequence', [ids], reuse=reuse):\n",
      "\u001b[0;31mValueError\u001b[0m: Must specify vocab size and embedding dimension when notreusing. Got vocab_size=None and embed_dim=1000"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"embed\") as scope:\n",
    "    input_embeddings = tf.contrib.layers.embed_sequence(encoder_inputs, vocab_size = opt.source_vocab_size,\n",
    "                                embed_dim=opt.source_embedding_size)\n",
    "    \n",
    "    decoder_targets_embeddings = tf.contrib.layers.embed_sequence(decoder_targets, vocab_size = opt.dest_vocab_size,\n",
    "                                embed_dim = opt.dest_embedding_size, scope = \"decoder_embeddings\")\n",
    "    \n",
    "    if opt.time_major:\n",
    "        input_embeddings = input_embeddings.swapaxes(0, 1) ## Change  Batch Major  to Time Major\n",
    "        decoder_targets_embeddings = decoder_targets_embeddings.swapaxes(0, 1)\n",
    "\n",
    "    \n",
    "    with tf.variable_scope(\"decoder_embeddings\", reuse = True):\n",
    "        decoder_embeddings_mat = tf.get_variable(\"embeddings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('encoder') as scope:\n",
    "    inputs_to_next = input_embeddings\n",
    "    encoder_fw_cells = []\n",
    "    encoder_bw_cells = []\n",
    "    \n",
    "    index = 0\n",
    "    for units in opt.encoder_hiddent_units_size:\n",
    "        encoder_fw_cells.append(tf.nn.rnn_cell.GRUCell(num_units = units, name = \"encoder_fw_\"+str(index)))\n",
    "        encoder_bw_cells.append(tf.nn.rnn_cell.GRUCell(num_units = units, name = \"encoder_bw_\"+str(index)))\n",
    "        index = index + 1\n",
    "    \n",
    "    for index in range(len(encoder_hiddent_units_size)):\n",
    "        ((encoder_fw_outputs,\n",
    "              encoder_bw_outputs),\n",
    "         (encoder_fw_final_state,\n",
    "              encoder_bw_final_state)) = tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_fw_cells[index],\n",
    "                                                    cell_bw=encoder_bw_cells[index],\n",
    "                                                    inputs=inputs_to_next,\n",
    "                                                    sequence_length=encoder_inputs_length,\n",
    "                                                    dtype=tf.float64, time_major=opt.time_major, scope = \"encoder_bidi_rnn\"+str(index))\n",
    "        \n",
    "        inpputs_to_next = tf.concat((encoder_fw_outputs,encoder_bw_outputs),2)\n",
    "\n",
    "\n",
    "    encoder_outputs = inputs_to_next\n",
    "    encoder_final_state = tf.concat((encoder_fw_final_state, encoder_bw_final_state), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##FOR LSTM      \n",
    "#     encoder_final_state_c = tf.concat(\n",
    "#                     (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "\n",
    "#     encoder_final_state_h = tf.concat(\n",
    "#                     (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "\n",
    "#     #TF Tuple used by LSTM Cells for state_size, zero_state, and output state.\n",
    "#     encoder_final_state = tf.nn.rnn_cell.LSTMStateTuple(\n",
    "#                                 c=encoder_final_state_c,\n",
    "#                                 h=encoder_final_state_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(helper, reuse = None):\n",
    "    with tf.variable_scope(\"decoder\", reuse = reuse) as scope:\n",
    "\n",
    "        decoder_cells = []\n",
    "\n",
    "        index  = 0\n",
    "        for units in decoder_hidden_units_size:\n",
    "            decoder_cells.append(tf.nn.rnn_cell.GRUCell(num_units = units, name = \"decoder_cell_\"+str(index)))\n",
    "            index = index + 1\n",
    "\n",
    "        decoder_stack = tf.nn.rnn_cell.MultiRNNCell(decoder_cells)\n",
    "\n",
    "        attention_mechanism = tf.contrib.seq2seq.LuongAttention(num_units=opt.attention_depth, \n",
    "                                                                memory=encoder_outputs,\n",
    "                                                                memory_sequence_length=None)\n",
    "        attn_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                    cell, attention_mechanism, alignment_history = False, attention_layer_size=opt.attention_layer_size)\n",
    "\n",
    "        out_cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "                    attn_cell, data.dest_vocab_size, reuse=reuse)\n",
    "\n",
    "        decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                    out_cell, helper=helper,\n",
    "                    initial_state=out_cell.zero_state(\n",
    "                        dtype=tf.float32, batch_size=batch_size))\n",
    "\n",
    "\n",
    "        outputs = tf.contrib.seq2seq.dynamic_decode(\n",
    "                    decoder=decoder, output_time_major=opt.time_major,\n",
    "                    impute_finished=True, maximum_iterations=None)         #maximum_iterations=dest_max_length\n",
    "        return outputs[0]    ##final_outputs, final_state, final_sequence_lengths\n",
    "        \n",
    "train_helper = tf.contrib.seq2seq.TrainingHelper(decoder_targets_embeddings, decoder_targets_length, time_major=opt.time_major)\n",
    "\n",
    "#batch_size = tf.unstack(tf.shape()) ## TODO\n",
    "start_tokens =  tf.fill([tf.cast(dest_start_token, tf.int32)], batch_size)\n",
    "infer_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(decoder_embeddings_mat, start_tokens=start_tokens, end_token=dest_end_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The encoder doesn't need GO & you can merge EOS & PAD (as long as sequence lengths are correct). Here are an example with a batch of 2 sentences for training:\n",
    "# (a) Encoder inputs (encoder lengths = [3, 2]):\n",
    "# a b c EOS\n",
    "# d e EOS EOS\n",
    "# (b) Decoder inputs (decoder lengths = [4, 3]):\n",
    "# GO 1 2 3\n",
    "# GO 4 5 EOS\n",
    "# (c) Decoder outputs (shift-by-1 of decoder inputs):\n",
    "# 1 2 3 EOS\n",
    "# 4 5 EOS EOS\n",
    "# (the first EOS is part of the loss)\n",
    "\n",
    "# During inference: we only have (a) + GO symbol fed to GreedyEmbeddingHelper, so it's important to make sure this GO is the same as the GO in (b).\n",
    "\n",
    "\n",
    "def get_src_tgt_datasaet(src_dataset,\n",
    "                 dest_dataset,\n",
    "                 src_lookup_table,\n",
    "                 dest_lookup_table,\n",
    "                 batch_size,\n",
    "                 #src_max_len=None,\n",
    "                 #dest_max_len=None,\n",
    "                 num_parallel_calls = 4\n",
    "                 reshuffle_each_iteration=True,\n",
    "                 buffer_size = None):\n",
    "    \n",
    "    if not buffer_size:\n",
    "        buffer_size = batch_size*1000\n",
    "        \n",
    "    src_tgt_dataset = tf.data.Dataset.zip((src_dataset, dest_dataset))\n",
    "    src_tgt_dataset = src_tgt_dataset.map(\n",
    "            lambda src,dest: (tf.string_split([src]).values, tf.string_split([dest]).values),\n",
    "            num_parallel_calls = num_parallel_calls\n",
    "                ).prefetch(buffer_size)\n",
    "    \n",
    "    src_tgt_dataset = src_tgt_dataset.map(\n",
    "            lambda src,dest:(tf.cast(src_lookup_table.lookup(src),tf.int32),\n",
    "                             tf.cast(dest_lookup_table.lookup(dest),tf.int32)), \n",
    "            num_parallel_calls = num_parallel_calls\n",
    "                ).prefetch(buffer_size)\n",
    "    \n",
    "    \n",
    "    src_eos_id = tf.cast(src_lookup_table.lookup(tf.constant(EOS)), tf.int32)\n",
    "    dest_eos_id = tf.cast(dest_lookup_table.lookup(tf.constant(EOS)), tf.int32)\n",
    "    dest_sos_id = tf.cast(dest_lookup_table.lookup(tf.constant(SOS)), tf.int32)\n",
    "    \n",
    "    src_tgt_dataset = src_tgt_dataset.map(\n",
    "            lambda src,dest:(tf.concat((src, [src_eos_id]), 0),\n",
    "                            tf.concat(([dest_sos_id], dest, [dest_eos_id]), 0)), \n",
    "            num_parallel_calls = num_parallel_calls\n",
    "                ).prefetch(buffer_size)\n",
    "    \n",
    "    src_tgt_dataset = src_tgt_dataset.map(\n",
    "            lambda src,dest:( 'src':  src, 'dest': dest, 'src_len': [tf.size(src)], 'dest_len':[tf.size(dest)]), \n",
    "            num_parallel_calls = num_parallel_calls\n",
    "                ).prefetch(buffer_size)\n",
    "    \n",
    "    src_tgt_dataset = src_tgt_dataset.repeat()\n",
    "    src_tgt_dataset = src_tgt_dataset.shuffle(buffer_size)\n",
    "    src_tgt_dataset = src_tgt_dataset.padded_batch(batch_size, \n",
    "                                        padded_shapes={'src': [None], 'dest': [None], 'src_len':[None], 'dest_len':[None]},           \n",
    "                                        padding_values = {'src': src_eos_id, 'dest': dest_eos_id, 'src_len':0, 'dest_len':0})\n",
    "    \n",
    "    return src_tgt_dataset\n",
    "    \n",
    "\n",
    "def get_reinitializable_iterator(src_tgt_dataset):\n",
    "    return tf.data.Iterator.from_structure(src_tgt_dataset.output_types, src_tgt_dataset.output_shapes)\n",
    "\n",
    "#next_element = iterator.get_next()\n",
    "\n",
    "#training_init_op = iterator.make_initializer(training_dataset)\n",
    "#validation_init_op = iterator.make_initializer(validation_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING and VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_decoder_outputs = decoder(train_helper)\n",
    "infer_decoder_outputs = decoder(infer_helper, reuse=True)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.tables_initializer())\n",
    "\n",
    "seq2seq_outputs = sess.run(training_decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.sequence_mask([1, 3, 2], 5)  # [[True, False, False, False, False],\n",
    "                                #  [True, True, True, False, False],\n",
    "                                #  [True, True, False, False, False]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
